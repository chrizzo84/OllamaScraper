{
  "scraped_at": "2026-01-17T05:08:11.778165+00:00",
  "models": [
    {
      "slug": "llama3.1",
      "capabilities": [
        "reasoning",
        "tools"
      ],
      "pulls": 108800000,
      "pulls_text": "108.8M \n  Downloads",
      "blurb": "llama3.1 Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes. 8b 70b 405b 93 Tags Updated 1 year ago",
      "name": "Llama 3.1 is a new state-of-the-art model from Meta available in 8B, 70B and 405B parameter sizes.",
      "description": "Readme Meta Llama 3.1 Llama 3.1 family of models available: 8B 70B 405B Llama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. The upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning capabilities. This enables Meta’s latest models to support advanced use cases, such as long-form text summarization, multilingual conversational agents, and coding assistants. Meta also has made changes to their license, allowing developers to use the outputs from Llama models, including the 405B model, to improve other models. Model evaluations For this release, Meta has evaluation the performance on over 150 benchmark datasets that span a wide range of languages. In addition, Meta performed extensive human evaluations that compare Llama 3.1 with competing models in real-world scenarios. Meta’s experimental evaluation suggests that our flagship model is competitive with leading foundation models across a range of tasks, including GPT-4, GPT-4o, and Claude 3.5 Sonnet. Additionally, Meta’s smaller models are competitive with closed and open models that have a similar number of parameters. References Meta AI Llama 3.1 launch blog post Write Preview ### Meta Llama 3.1 ![image.png](https://ollama.com/assets/mchiang0610/mikey3.1/4d0cab8e-952b-4c75-b110-1514d8db8fae) **Llama 3.1** family of models available: - **8B** - **70B** - **405B** Llama 3.1 405B is the first openly available model that rivals the top AI models when it comes to state-of-the-art capabilities in general knowledge, steerability, math, tool use, and multilingual translation. The upgraded versions of the 8B and 70B models are multilingual and have a significantly longer context length of 128K, state-of-the-art tool use, and overall stronger reasoning ca",
      "variants": [
        {
          "tag": "llama3.1:latest",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b",
          "size_text": "243GB",
          "size_bytes": 260919263232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:8b-text-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-instruct-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:70b-text-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q2_K",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q3_K_S",
          "size_text": "175GB",
          "size_bytes": 187904819200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q3_K_M",
          "size_text": "195GB",
          "size_bytes": 209379655680,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q3_K_L",
          "size_text": "213GB",
          "size_bytes": 228707008512,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q4_0",
          "size_text": "229GB",
          "size_bytes": 245886877696,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q4_1",
          "size_text": "254GB",
          "size_bytes": 272730423296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q4_K_S",
          "size_text": "231GB",
          "size_bytes": 248034361344,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q4_K_M",
          "size_text": "243GB",
          "size_bytes": 260919263232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q5_0",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q5_1",
          "size_text": "305GB",
          "size_bytes": 327491256320,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q5_K_S",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q5_K_M",
          "size_text": "287GB",
          "size_bytes": 308163903488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q6_K",
          "size_text": "333GB",
          "size_bytes": 357556027392,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-q8_0",
          "size_text": "431GB",
          "size_bytes": 462782726144,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-instruct-fp16",
          "size_text": "812GB",
          "size_bytes": 871878361088,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q2_K",
          "size_text": "149GB",
          "size_bytes": 159987531776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q3_K_S",
          "size_text": "175GB",
          "size_bytes": 187904819200,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q3_K_M",
          "size_text": "195GB",
          "size_bytes": 209379655680,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q3_K_L",
          "size_text": "213GB",
          "size_bytes": 228707008512,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q4_0",
          "size_text": "229GB",
          "size_bytes": 245886877696,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q4_1",
          "size_text": "254GB",
          "size_bytes": 272730423296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q4_K_S",
          "size_text": "231GB",
          "size_bytes": 248034361344,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q4_K_M",
          "size_text": "243GB",
          "size_bytes": 260919263232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q5_0",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q5_1",
          "size_text": "305GB",
          "size_bytes": 327491256320,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q5_K_S",
          "size_text": "279GB",
          "size_bytes": 299573968896,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q5_K_M",
          "size_text": "287GB",
          "size_bytes": 308163903488,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q6_K",
          "size_text": "333GB",
          "size_bytes": 357556027392,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-q8_0",
          "size_text": "431GB",
          "size_bytes": 462782726144,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.1:405b-text-fp16",
          "size_text": "812GB",
          "size_bytes": 871878361088,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 93
    },
    {
      "slug": "deepseek-r1",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 76400000,
      "pulls_text": "76.4M \n  Downloads",
      "blurb": "deepseek-r1 DeepSeek-R1 is a family of open models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro. 1.5b 7b 8b 14b 32b 70b 671b 35 Tags Updated 6 months ago",
      "name": "DeepSeek-R1 is a family of open reasoning models with performance approaching that of leading models, such as O3 and Gemini 2.5 Pro.",
      "description": "Readme DeepSeek-R1 has received a minor version upgrade to DeepSeek-R1-0528 for the 8 billion parameter distilled model and the full 671 billion parameter model. In this update, DeepSeek R1 has significantly improved its reasoning and inference capabilities. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro. Models DeepSeek-R1-0528-Qwen3-8B ollama run deepseek-r1 DeepSeek-R1 ollama run deepseek-r1:671b Note: to update the model from an older version, run ollama pull deepseek-r1 Distilled models DeepSeek team has demonstrated that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. Below are the models created via fine-tuning against several dense models widely used in the research community using reasoning data generated by DeepSeek-R1. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. DeepSeek-R1-0528-Qwen3-8B ollama run deepseek-r1:8b DeepSeek-R1-Distill-Qwen-1.5B ollama run deepseek-r1:1.5b DeepSeek-R1-Distill-Qwen-7B ollama run deepseek-r1:7b DeepSeek-R1-Distill-Qwen-14B ollama run deepseek-r1:14b DeepSeek-R1-Distill-Qwen-32B ollama run deepseek-r1:32b DeepSeek-R1-Distill-Llama-70B ollama run deepseek-r1:70b License The model weights are licensed under the MIT License. DeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that: The Qwen distilled models are derived from Qwen-2.5 series, which are originally licensed under Apache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1. The Llama 8B distilled model is derived from Llama3.1-8B-Bas",
      "variants": [
        {
          "tag": "deepseek-r1:latest",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:1.5b",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:14b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:70b",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:1.5b-qwen-distill-q4_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:1.5b-qwen-distill-q8_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:1.5b-qwen-distill-fp16",
          "size_text": "3.6GB",
          "size_bytes": 3865470566,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:7b-qwen-distill-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:7b-qwen-distill-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:7b-qwen-distill-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-0528-qwen3-q4_K_M",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-0528-qwen3-q8_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-0528-qwen3-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-llama-distill-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-llama-distill-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:8b-llama-distill-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:14b-qwen-distill-q4_K_M",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:14b-qwen-distill-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:14b-qwen-distill-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:32b-qwen-distill-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:32b-qwen-distill-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:32b-qwen-distill-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:70b-llama-distill-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:70b-llama-distill-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:70b-llama-distill-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-0528-q4_K_M",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-0528-q8_0",
          "size_text": "713GB",
          "size_bytes": 765577920512,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-0528-fp16",
          "size_text": null,
          "size_bytes": null,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-q4_K_M",
          "size_text": "404GB",
          "size_bytes": 433791696896,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-q8_0",
          "size_text": "713GB",
          "size_bytes": 765577920512,
          "context": "160K",
          "input": "Text"
        },
        {
          "tag": "deepseek-r1:671b-fp16",
          "size_text": null,
          "size_bytes": null,
          "context": "160K",
          "input": "Text"
        }
      ],
      "tags_count": 35
    },
    {
      "slug": "llama3.2",
      "capabilities": [
        "tools"
      ],
      "pulls": 53100000,
      "pulls_text": "53.1M \n  Downloads",
      "blurb": "llama3.2 Meta's Llama 3.2 goes small with 1B and 3B models. 1b 3b 63 Tags Updated 1 year ago",
      "name": "Meta's Llama 3.2 goes small with 1B and 3B models.",
      "description": "Readme The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks. Sizes 3B parameters (default) The 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as: Following instructions Summarization Prompt rewriting Tool use ollama run llama3.2 1B parameters The 1B model is competitive with other 1-3B parameter models. It’s use cases include: Personal information management Multilingual knowledge retrieval Rewriting tasks running locally on edge ollama run llama3.2:1b Benchmarks Supported Languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai are officially supported. Llama 3.2 has been trained on a broader collection of languages than these 8 supported languages. Write Preview <img src=\"/assets/library/llama3.2/be01fadf-7fbd-404d-929b-50a77249b030\" width=\"280\" /> The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out). The Llama 3.2 instruction-tuned text only models are optimized for multilingual dialogue use cases, including agentic retrieval and summarization tasks. They outperform many of the available open source and closed chat models on common industry benchmarks. ## Sizes ### 3B parameters (default) The 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as: * Following instructions * Summarization * Prompt rewriting * Tool use ``` ollama run llama3.2 ``` ### 1B parameters The 1B model is competitive with other 1-3B parameter models. It's use cases include: * Personal information management",
      "variants": [
        {
          "tag": "llama3.2:latest",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q2_K",
          "size_text": "581MB",
          "size_bytes": 609222656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q3_K_S",
          "size_text": "642MB",
          "size_bytes": 673185792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q3_K_M",
          "size_text": "691MB",
          "size_bytes": 724566016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q3_K_L",
          "size_text": "733MB",
          "size_bytes": 768606208,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q4_0",
          "size_text": "771MB",
          "size_bytes": 808452096,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q4_1",
          "size_text": "832MB",
          "size_bytes": 872415232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q4_K_S",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q4_K_M",
          "size_text": "808MB",
          "size_bytes": 847249408,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q5_0",
          "size_text": "893MB",
          "size_bytes": 936378368,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q5_1",
          "size_text": "953MB",
          "size_bytes": 999292928,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q5_K_S",
          "size_text": "893MB",
          "size_bytes": 936378368,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q5_K_M",
          "size_text": "912MB",
          "size_bytes": 956301312,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q6_K",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-q8_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-instruct-fp16",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q2_K",
          "size_text": "581MB",
          "size_bytes": 609222656,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q3_K_S",
          "size_text": "642MB",
          "size_bytes": 673185792,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q3_K_M",
          "size_text": "691MB",
          "size_bytes": 724566016,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q3_K_L",
          "size_text": "733MB",
          "size_bytes": 768606208,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q4_0",
          "size_text": "771MB",
          "size_bytes": 808452096,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q4_1",
          "size_text": "832MB",
          "size_bytes": 872415232,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q4_K_S",
          "size_text": "776MB",
          "size_bytes": 813694976,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q4_K_M",
          "size_text": "808MB",
          "size_bytes": 847249408,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q5_0",
          "size_text": "893MB",
          "size_bytes": 936378368,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q5_1",
          "size_text": "953MB",
          "size_bytes": 999292928,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q5_K_S",
          "size_text": "893MB",
          "size_bytes": 936378368,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q5_K_M",
          "size_text": "912MB",
          "size_bytes": 956301312,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q6_K",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-q8_0",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:1b-text-fp16",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q3_K_L",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q4_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q4_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q4_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q4_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q5_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q5_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q5_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q5_K_M",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q6_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-q8_0",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-instruct-fp16",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q3_K_M",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q3_K_L",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q4_0",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q4_1",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q4_K_S",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q4_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q5_0",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q5_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q5_K_S",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q5_K_M",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q6_K",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-q8_0",
          "size_text": "3.4GB",
          "size_bytes": 3650722201,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "llama3.2:3b-text-fp16",
          "size_text": "6.4GB",
          "size_bytes": 6871947673,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 63
    },
    {
      "slug": "nomic-embed-text",
      "capabilities": [
        "embedding"
      ],
      "pulls": 50200000,
      "pulls_text": "50.2M \n  Downloads",
      "blurb": "nomic-embed-text A high-performing open model with a large token context window. 3 Tags Updated 1 year ago",
      "name": "A high-performing open embedding model with a large token context window.",
      "description": "Readme Note: this model requires Ollama 0.1.26 or later. Download it here . It can only be used to generate embeddings. nomic-embed-text is a large context length text encoder that surpasses OpenAI text-embedding-ada-002 and text-embedding-3-small performance on short and long context tasks. Usage This model is an embedding model, meaning it can only be used to generate embeddings. REST API curl http://localhost:11434/api/embeddings -d '{ \"model\": \"nomic-embed-text\", \"prompt\": \"The sky is blue because of Rayleigh scattering\" }' Python library ollama.embeddings(model='nomic-embed-text', prompt='The sky is blue because of rayleigh scattering') Javascript library ollama.embeddings({ model: 'nomic-embed-text', prompt: 'The sky is blue because of rayleigh scattering' }) References HuggingFace Blog Post Write Preview ![nomic_logo](https://github.com/ollama/ollama/assets/251292/bf242e43-3c1a-4590-887d-abcab76cb304) > Note: this model requires Ollama 0.1.26 or later. [Download it here](https://ollama.com/download). It can only be used to generate embeddings. `nomic-embed-text` is a large context length text encoder that surpasses OpenAI `text-embedding-ada-002` and `text-embedding-3-small` performance on short and long context tasks. ## Usage This model is an embedding model, meaning it can only be used to generate embeddings. ### REST API ``` curl http://localhost:11434/api/embeddings -d '{ \"model\": \"nomic-embed-text\", \"prompt\": \"The sky is blue because of Rayleigh scattering\" }' ``` ### Python library ``` ollama.embeddings(model='nomic-embed-text', prompt='The sky is blue because of rayleigh scattering') ``` ### Javascript library ``` ollama.embeddings({ model: 'nomic-embed-text', prompt: 'The sky is blue because of rayleigh scattering' }) ``` ## References [HuggingFace](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5) [Blog Post](https://blog.nomic.ai/posts/nomic-embed-text-v1) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "nomic-embed-text:latest",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nomic-embed-text:v1.5",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        },
        {
          "tag": "nomic-embed-text:137m-v1.5-fp16",
          "size_text": "274MB",
          "size_bytes": 287309824,
          "context": "2K",
          "input": "Text"
        }
      ],
      "tags_count": 3
    },
    {
      "slug": "gemma3",
      "capabilities": [
        "multimodal",
        "reasoning",
        "vision"
      ],
      "pulls": 30000000,
      "pulls_text": "30M \n  Downloads",
      "blurb": "gemma3 The current, most capable model that runs on a single GPU. cloud 270m 1b 4b 12b 27b 29 Tags Updated 1 month ago",
      "name": "The current, most capable model that runs on a single GPU.",
      "description": "Readme This model requires Ollama 0.6 or later. Download Ollama Gemma is a lightweight, family of models from Google built on Gemini technology. The Gemma 3 models are multimodal—processing text and images—and feature a 128K context window with support for over 140 languages. Available in 270M, 1B, 4B, 12B, and 27B parameter sizes, they excel in tasks like question answering, summarization, and reasoning, while their compact design allows deployment on resource-limited devices. Models Text 270M parameter model (32k context window) ollama run gemma3:270m 1B parameter model (32k context window) ollama run gemma3:1b Multimodal (Vision) 4B parameter model (128k context window) ollama run gemma3:4b 12B parameter model (128k context window) ollama run gemma3:12b 27B parameter model (128k context window) ollama run gemma3:27b Quantization aware trained models (QAT) The quantization aware trained Gemma 3 models preserves similar quality as half precision models (BF16) while maintaining a lower memory footprint (3x less compared to non-quantized models). 1B parameter model ollama run gemma3:1b-it-qat 4B parameter model ollama run gemma3:4b-it-qat 12B parameter model ollama run gemma3:12b-it-qat 27B parameter model ollama run gemma3:27b-it-qat Evaluation Benchmark Results Gemma 3 270M Benchmark n-shot Gemma 3 270m instruction tuned HellaSwag 0-shot 37.7 PIQA 0-shot 66.2 ARC-c 0-shot 28.2 WinoGrande 0-shot 52.3 BIG-Bench Hard few-shot 26.7 IF Eval 0-shot 51.2 These models were evaluated against a large collection of different datasets and metrics to cover different aspects of text generation: Reasoning, logic and code capabilities Benchmark Metric Gemma 3 PT 1B Gemma 3 PT 4B Gemma 3 PT 12B Gemma 3 PT 27B HellaSwag 10-shot 62.3 77.2 84.2 85.6 BoolQ 0-shot 63.2 72.3 78.8 82.4 PIQA 0-shot 73.8 79.6 81.8 83.3 SocialIQA 0-shot 48.9 51.9 53.4 54.9 TriviaQA 5-shot 39.8 65.8 78.2 85.5 Natural Questions 5-shot 9.48 20.0 31.4 36.1 ARC-c 25-shot 38.4 56.2 68.9 70.6 ARC-e 0-shot 73.0 82.4",
      "variants": [
        {
          "tag": "gemma3:latest",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m",
          "size_text": "292MB",
          "size_bytes": 306184192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b",
          "size_text": "815MB",
          "size_bytes": 854589440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m-it-qat",
          "size_text": "241MB",
          "size_bytes": 252706816,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m-it-q8_0",
          "size_text": "292MB",
          "size_bytes": 306184192,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m-it-fp16",
          "size_text": "543MB",
          "size_bytes": 569376768,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:270m-it-bf16",
          "size_text": "543MB",
          "size_bytes": 569376768,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b-it-qat",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b-it-q4_K_M",
          "size_text": "815MB",
          "size_bytes": 854589440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b-it-q8_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:1b-it-fp16",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-it-qat",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-it-q4_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-it-q8_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:4b-it-fp16",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-it-qat",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-it-q4_K_M",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-it-q8_0",
          "size_text": "13GB",
          "size_bytes": 13958643712,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:12b-it-fp16",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-it-qat",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-it-q4_K_M",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-it-q8_0",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gemma3:27b-it-fp16",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 29
    },
    {
      "slug": "mistral",
      "capabilities": [
        "tools"
      ],
      "pulls": 24200000,
      "pulls_text": "24.2M \n  Downloads",
      "blurb": "mistral The 7B model released by Mistral AI, updated to version 0.3. 7b 84 Tags Updated 6 months ago",
      "name": "The 7B model released by Mistral AI, updated to version 0.3.",
      "description": "Readme Mistral is a 7B parameter model, distributed with the Apache license. It is available in both instruct (instruction following) and text completion. The Mistral AI team has noted that Mistral 7B: Outperforms Llama 2 13B on all benchmarks Outperforms Llama 1 34B on many benchmarks Approaches CodeLlama 7B performance on code, while remaining good at English tasks Versions Tag Date Notes v0.3 latest 05/22/2024 A new version of Mistral 7B that supports function calling. v0.2 03/23/2024 A minor release of Mistral 7B v0.1 09/27/2023 Initial release Function calling Mistral 0.3 supports function calling with Ollama’s raw mode . Example raw prompt [AVAILABLE_TOOLS] [{\"type\": \"function\", \"function\": {\"name\": \"get_current_weather\", \"description\": \"Get the current weather\", \"parameters\": {\"type\": \"object\", \"properties\": {\"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"}, \"format\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"], \"description\": \"The temperature unit to use. Infer this from the users location.\"}}, \"required\": [\"location\", \"format\"]}}}][/AVAILABLE_TOOLS][INST] What is the weather like today in San Francisco [/INST] Example response [TOOL_CALLS] [{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"San Francisco, CA\", \"format\": \"celsius\"}}] For more information on raw mode, see the API documentation . Variations instruct Instruct models follow instructions text Text models are the base foundation model without any fine-tuning for conversations, and are best used for simple text completion. Usage CLI Instruct: ollama run mistral API Example: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"mistral\", \"prompt\":\"Here is a story about llamas eating grass\" }' References HuggingFace Mistral AI News Release Write Preview <img src=\"https://github.com/jmorganca/ollama/assets/3325447/d6be0694-eb35-417b-8f08-47d3b6c2a171\" width=\"200\"/> Mistral is a 7B parameter model, distributed with the Apache licens",
      "variants": [
        {
          "tag": "mistral:latest",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:instruct",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:text",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:v0.1",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:v0.2",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:v0.3",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.2-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-instruct-v0.3-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q2_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q2_K",
          "size_text": "2.7GB",
          "size_bytes": 2899102924,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q3_K_S",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q3_K_M",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q3_K_L",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q4_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q4_1",
          "size_text": "4.6GB",
          "size_bytes": 4939212390,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q4_K_S",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q4_K_M",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q5_0",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q5_1",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q5_K_S",
          "size_text": "5.0GB",
          "size_bytes": 5368709120,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q5_K_M",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q6_K",
          "size_text": "5.9GB",
          "size_bytes": 6335076761,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-q8_0",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "16K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "mistral:7b-text-v0.2-fp16",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "16K",
          "input": "Text"
        }
      ],
      "tags_count": 84
    },
    {
      "slug": "qwen2.5",
      "capabilities": [
        "tools"
      ],
      "pulls": 19300000,
      "pulls_text": "19.3M \n  Downloads",
      "blurb": "qwen2.5 Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support. 0.5b 1.5b 3b 7b 14b 32b 72b 133 Tags Updated 1 year ago",
      "name": "Qwen2.5 models are pretrained on Alibaba's latest large-scale dataset, encompassing up to 18 trillion tokens. The model supports up to 128K tokens and has multilingual support.",
      "description": "Readme Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2: It possesses significantly more knowledge and has greatly enhanced capabilities in coding and mathematics , due to specialized expert models in these domains. It demonstrates significant advancements in instruction following , long-text generation (over 8K tokens), understanding structured data (e.g., tables), and generating structured outputs , especially in JSON format. It is also more resilient to diverse system prompts , improving role-play and condition-setting for chatbots. It supports long contexts of up to 128K tokens and can generate up to 8K tokens. It offers multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more. Please note: all models except the 3B and 72B are released under the Apache 2.0 license, while the 3B and 72B models are under the Qwen license. References GitHub Blog post HuggingFace Write Preview <img src=\"https://ollama.com/assets/library/qwen2.5/4b4f719f-c327-489e-8dc1-89a455c21e89\" width=\"320\" /> Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, a range of base language models and instruction-tuned models are released, with sizes ranging from 0.5 to 72 billion parameters. Qwen2.5 introduces the following improvements over Qwen2: - It possesses **significantly more knowledge** and has greatly enhanced capabilities in **coding** and **mathematics**, due to specialized expert models in these domains. - It demonstrates significant advancements in **instruction following**, **long-text generation** (over 8K tokens), **understanding structured data** (e.g., tables), and **generating structured outputs**, especially in JSON format. It is al",
      "variants": [
        {
          "tag": "qwen2.5:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q2_K",
          "size_text": "339MB",
          "size_bytes": 355467264,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q3_K_S",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q3_K_M",
          "size_text": "355MB",
          "size_bytes": 372244480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q3_K_L",
          "size_text": "369MB",
          "size_bytes": 386924544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q4_0",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q4_1",
          "size_text": "375MB",
          "size_bytes": 393216000,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q4_K_S",
          "size_text": "385MB",
          "size_bytes": 403701760,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q4_K_M",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q5_0",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q5_1",
          "size_text": "419MB",
          "size_bytes": 439353344,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q5_K_S",
          "size_text": "413MB",
          "size_bytes": 433061888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-base-q8_0",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q2_K",
          "size_text": "339MB",
          "size_bytes": 355467264,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q3_K_S",
          "size_text": "338MB",
          "size_bytes": 354418688,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q3_K_M",
          "size_text": "355MB",
          "size_bytes": 372244480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q3_K_L",
          "size_text": "369MB",
          "size_bytes": 386924544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q4_0",
          "size_text": "352MB",
          "size_bytes": 369098752,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q4_1",
          "size_text": "375MB",
          "size_bytes": 393216000,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q4_K_S",
          "size_text": "385MB",
          "size_bytes": 403701760,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q4_K_M",
          "size_text": "398MB",
          "size_bytes": 417333248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q5_0",
          "size_text": "397MB",
          "size_bytes": 416284672,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q5_1",
          "size_text": "419MB",
          "size_bytes": 439353344,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q5_K_S",
          "size_text": "413MB",
          "size_bytes": 433061888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q5_K_M",
          "size_text": "420MB",
          "size_bytes": 440401920,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q6_K",
          "size_text": "506MB",
          "size_bytes": 530579456,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-q8_0",
          "size_text": "531MB",
          "size_bytes": 556793856,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:0.5b-instruct-fp16",
          "size_text": "994MB",
          "size_bytes": 1042284544,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q2_K",
          "size_text": "676MB",
          "size_bytes": 708837376,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q3_K_S",
          "size_text": "761MB",
          "size_bytes": 797966336,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q3_K_M",
          "size_text": "824MB",
          "size_bytes": 864026624,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q3_K_L",
          "size_text": "880MB",
          "size_bytes": 922746880,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q4_0",
          "size_text": "935MB",
          "size_bytes": 980418560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q4_1",
          "size_text": "1.0GB",
          "size_bytes": 1073741824,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q4_K_S",
          "size_text": "940MB",
          "size_bytes": 985661440,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q4_K_M",
          "size_text": "986MB",
          "size_bytes": 1033895936,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q5_0",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q5_1",
          "size_text": "1.2GB",
          "size_bytes": 1288490188,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q5_K_S",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q5_K_M",
          "size_text": "1.1GB",
          "size_bytes": 1181116006,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q6_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-q8_0",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:1.5b-instruct-fp16",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q2_K",
          "size_text": "1.3GB",
          "size_bytes": 1395864371,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q3_K_S",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q3_K_M",
          "size_text": "1.6GB",
          "size_bytes": 1717986918,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q3_K_L",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q4_0",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q4_1",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q4_K_S",
          "size_text": "1.8GB",
          "size_bytes": 1932735283,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q4_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q5_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q5_1",
          "size_text": "2.3GB",
          "size_bytes": 2469606195,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q5_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q5_K_M",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q6_K",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-q8_0",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:3b-instruct-fp16",
          "size_text": "6.2GB",
          "size_bytes": 6657199308,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q2_K",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q3_K_S",
          "size_text": "3.5GB",
          "size_bytes": 3758096384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q3_K_M",
          "size_text": "3.8GB",
          "size_bytes": 4080218931,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q3_K_L",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q4_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q4_1",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q4_K_S",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q4_K_M",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q5_0",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q5_1",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q5_K_S",
          "size_text": "5.3GB",
          "size_bytes": 5690831667,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q5_K_M",
          "size_text": "5.4GB",
          "size_bytes": 5798205849,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q6_K",
          "size_text": "6.3GB",
          "size_bytes": 6764573491,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-q8_0",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:7b-instruct-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q2_K",
          "size_text": "5.8GB",
          "size_bytes": 6227702579,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q3_K_S",
          "size_text": "6.7GB",
          "size_bytes": 7194070220,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q3_K_M",
          "size_text": "7.3GB",
          "size_bytes": 7838315315,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q3_K_L",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q4_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q4_1",
          "size_text": "9.4GB",
          "size_bytes": 10093173145,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q4_K_S",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q4_K_M",
          "size_text": "9.0GB",
          "size_bytes": 9663676416,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q5_0",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q5_1",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q5_K_S",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q5_K_M",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q6_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:14b-instruct-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q2_K",
          "size_text": "12GB",
          "size_bytes": 12884901888,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q3_K_S",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q3_K_M",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q3_K_L",
          "size_text": "17GB",
          "size_bytes": 18253611008,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q4_0",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q4_1",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q4_K_S",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q5_0",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q5_1",
          "size_text": "25GB",
          "size_bytes": 26843545600,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q5_K_S",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q5_K_M",
          "size_text": "23GB",
          "size_bytes": 24696061952,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q6_K",
          "size_text": "27GB",
          "size_bytes": 28991029248,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:32b-instruct-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q2_K",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q3_K_S",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q3_K_M",
          "size_text": "38GB",
          "size_bytes": 40802189312,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q3_K_L",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q4_0",
          "size_text": "41GB",
          "size_bytes": 44023414784,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q4_1",
          "size_text": "46GB",
          "size_bytes": 49392123904,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q4_K_S",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q4_K_M",
          "size_text": "47GB",
          "size_bytes": 50465865728,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q5_0",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q5_1",
          "size_text": "55GB",
          "size_bytes": 59055800320,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q5_K_S",
          "size_text": "51GB",
          "size_bytes": 54760833024,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q5_K_M",
          "size_text": "54GB",
          "size_bytes": 57982058496,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q6_K",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-q8_0",
          "size_text": "77GB",
          "size_bytes": 82678120448,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "qwen2.5:72b-instruct-fp16",
          "size_text": "145GB",
          "size_bytes": 155692564480,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 133
    },
    {
      "slug": "qwen3",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 17400000,
      "pulls_text": "17.4M \n  Downloads",
      "blurb": "qwen3 Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. 0.6b 1.7b 4b 8b 14b 30b 32b 235b 58 Tags Updated 3 months ago",
      "name": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models.",
      "description": "Readme Qwen 3 is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models: New 30B model ollama run qwen3:30b New 235B model ollama run qwen3:235b Overview The Qwen 3 family is a comprehensive suite of dense and mixture-of-experts (MoE) models. The flagship model, Qwen3-235B-A22B , achieves competitive results in benchmark evaluations of coding, math, general capabilities, etc., when compared to other top-tier models such as DeepSeek-R1, o1, o3-mini, Grok-3, and Gemini-2.5-Pro. Additionally, the small MoE model, Qwen3-30B-A3B , outcompetes QwQ-32B with 10 times of activated parameters, and even a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct. Significantly enhancement in its reasoning capabilities , surpassing previous QwQ (in thinking mode) and Qwen2.5 instruct models (in non-thinking mode) on mathematics, code generation, and commonsense logical reasoning. Superior human preference alignment , excelling in creative writing, role-playing, multi-turn dialogues, and instruction following, to deliver a more natural, engaging, and immersive conversational experience. Expertise in agent capabilities , enabling precise integration with external tools in both thinking and unthinking modes and achieving leading performance among open-source models in complex agent-based tasks. Support of 100+ languages and dialects with strong capabilities for multilingual instruction following and translation . Reference Blog Write Preview ![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522) **Qwen 3** is the latest generation of large language models in Qwen series, with newly updated versions of the 30B and 235B models: ### New 30B model ``` ollama run qwen3:30b ``` ![Qwen3-30B-A3B-Instruct-2507.jpg](/assets/library/qwen3/bc0ddfea-95b5-49fc-a36e-c817f98a5de0) ### New 235B model ``` ollama run qwen3:235b ``` ![0d7zztq4GB7G2ZYowO-dQ.jpg](/assets/library/qwen3/8426a459-dd88-49cd-",
      "variants": [
        {
          "tag": "qwen3:latest",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:0.6b",
          "size_text": "523MB",
          "size_bytes": 548405248,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:1.7b",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:8b",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:14b",
          "size_text": "9.3GB",
          "size_bytes": 9985798963,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:32b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:0.6b-q4_K_M",
          "size_text": "523MB",
          "size_bytes": 548405248,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:0.6b-q8_0",
          "size_text": "832MB",
          "size_bytes": 872415232,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:0.6b-fp16",
          "size_text": "1.5GB",
          "size_bytes": 1610612736,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:1.7b-q4_K_M",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:1.7b-q8_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:1.7b-fp16",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-instruct",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-instruct-2507-q4_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-instruct-2507-q8_0",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-instruct-2507-fp16",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-thinking",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-thinking-2507-q4_K_M",
          "size_text": "2.5GB",
          "size_bytes": 2684354560,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-thinking-2507-q8_0",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-thinking-2507-fp16",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-q4_K_M",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-q8_0",
          "size_text": "4.4GB",
          "size_bytes": 4724464025,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:4b-fp16",
          "size_text": "8.1GB",
          "size_bytes": 8697308774,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:8b-q4_K_M",
          "size_text": "5.2GB",
          "size_bytes": 5583457484,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:8b-q8_0",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:8b-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:14b-q4_K_M",
          "size_text": "9.3GB",
          "size_bytes": 9985798963,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:14b-q8_0",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:14b-fp16",
          "size_text": "30GB",
          "size_bytes": 32212254720,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-instruct-2507-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-instruct-2507-q8_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-thinking-2507-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-q8_0",
          "size_text": "33GB",
          "size_bytes": 35433480192,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-thinking-2507-q8_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-fp16",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-instruct-2507-fp16",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-a3b-thinking-2507-fp16",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-instruct",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:30b-thinking",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:32b-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:32b-q8_0",
          "size_text": "35GB",
          "size_bytes": 37580963840,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:32b-fp16",
          "size_text": "66GB",
          "size_bytes": 70866960384,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-instruct-2507-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-instruct-2507-q8_0",
          "size_text": "250GB",
          "size_bytes": 268435456000,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-thinking-2507-q4_K_M",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-q8_0",
          "size_text": "250GB",
          "size_bytes": 268435456000,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-thinking-2507-q8_0",
          "size_text": "250GB",
          "size_bytes": 268435456000,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-fp16",
          "size_text": "470GB",
          "size_bytes": 504658657280,
          "context": "40K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-a22b-thinking-2507-fp16",
          "size_text": "470GB",
          "size_bytes": 504658657280,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-instruct",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3:235b-thinking",
          "size_text": "142GB",
          "size_bytes": 152471339008,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 58
    },
    {
      "slug": "phi3",
      "capabilities": [
        "multimodal",
        "reasoning"
      ],
      "pulls": 15600000,
      "pulls_text": "15.6M \n  Downloads",
      "blurb": "phi3 Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft. 3.8b 14b 72 Tags Updated 1 year ago",
      "name": "Phi-3 is a family of lightweight 3B (Mini) and 14B (Medium) state-of-the-art open models by Microsoft.",
      "description": "Readme Phi-3 is a family of open AI models developed by Microsoft. Parameter sizes Phi-3 Mini – 3B parameters – ollama run phi3:mini Phi-3 Medium – 14B parameters – ollama run phi3:medium Context window sizes Note: the 128k version of this model requires Ollama 0.1.39 or later. 4k ollama run phi3:mini ollama run phi3:medium 128k ollama run phi3:medium-128k Phi-3 Mini Phi-3 Mini is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. The model has underwent a post-training process that incorporates both supervised fine-tuning and direct preference optimization to ensure precise instruction adherence and robust safety measures. When assessed against benchmarks testing common sense, language understanding, math, code, long context and logical reasoning, Phi-3 Mini-4K-Instruct showcased a robust and state-of-the-art performance among models with less than 13 billion parameters. Phi-3 Medium Phi-3 Medium is a 14B parameter language model, and outperforms Gemini 1.0 Pro. Intended Uses Primary use cases The model is intended for commercial and research use in English. The model provides uses for applications which require 1) memory/compute constrained environments 2) latency bound scenarios 3) strong reasoning (especially math and logic) 4) long context Our model is designed to accelerate research on language and multimodal models, for use as a building block for generative AI powered features. Use case considerations Our models are not specifically designed or evaluated for all downstream purposes. Developers should consider common limitations of language models as they select use cases, and evaluate and mitigate for accuracy, safety, and fariness before using within a specific downstream use case, particularly for high risk scenarios. Developers should be aware of and adhere to applicable l",
      "variants": [
        {
          "tag": "phi3:latest",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:instruct",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:medium",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:mini",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-instruct",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q3_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q3_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q3_K_L",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q4_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q4_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q4_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q4_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q5_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q5_1",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q5_K_S",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q5_K_M",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q6_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-q8_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-128k-instruct-fp16",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q2_K",
          "size_text": "1.4GB",
          "size_bytes": 1503238553,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q3_K_S",
          "size_text": "1.7GB",
          "size_bytes": 1825361100,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q3_K_M",
          "size_text": "2.0GB",
          "size_bytes": 2147483648,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q3_K_L",
          "size_text": "2.1GB",
          "size_bytes": 2254857830,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q4_0",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q4_1",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q4_K_S",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q4_K_M",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q5_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q5_1",
          "size_text": "2.9GB",
          "size_bytes": 3113851289,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q5_K_S",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q5_K_M",
          "size_text": "2.8GB",
          "size_bytes": 3006477107,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q6_K",
          "size_text": "3.1GB",
          "size_bytes": 3328599654,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-q8_0",
          "size_text": "4.1GB",
          "size_bytes": 4402341478,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:3.8b-mini-4k-instruct-fp16",
          "size_text": "7.6GB",
          "size_bytes": 8160437862,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-instruct",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q2_K",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q3_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q3_K_M",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q3_K_L",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q4_0",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q4_1",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q4_K_S",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q4_K_M",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q5_0",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q5_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q5_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q5_K_M",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-128k-instruct-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q2_K",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q3_K_S",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q3_K_M",
          "size_text": "6.9GB",
          "size_bytes": 7408818585,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q3_K_L",
          "size_text": "7.5GB",
          "size_bytes": 8053063680,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q4_0",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q4_1",
          "size_text": "8.8GB",
          "size_bytes": 9448928051,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q4_K_S",
          "size_text": "8.0GB",
          "size_bytes": 8589934592,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q4_K_M",
          "size_text": "8.6GB",
          "size_bytes": 9234179686,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q5_0",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q5_1",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q5_K_S",
          "size_text": "9.6GB",
          "size_bytes": 10307921510,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q5_K_M",
          "size_text": "10GB",
          "size_bytes": 10737418240,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q6_K",
          "size_text": "11GB",
          "size_bytes": 11811160064,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:14b-medium-4k-instruct-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:medium-128k",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:medium-4k",
          "size_text": "7.9GB",
          "size_bytes": 8482560409,
          "context": "4K",
          "input": "Text"
        },
        {
          "tag": "phi3:mini-128k",
          "size_text": "2.2GB",
          "size_bytes": 2362232012,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "phi3:mini-4k",
          "size_text": "2.4GB",
          "size_bytes": 2576980377,
          "context": "4K",
          "input": "Text"
        }
      ],
      "tags_count": 72
    },
    {
      "slug": "llama3",
      "capabilities": [],
      "pulls": 13800000,
      "pulls_text": "13.8M \n  Downloads",
      "blurb": "llama3 Meta Llama 3: The most capable openly available LLM to date 8b 70b 68 Tags Updated 1 year ago",
      "name": "Meta Llama 3: The most capable openly available LLM to date",
      "description": "Readme Llama 3 The most capable openly available LLM to date. Meta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both 8B and 70B parameter sizes (pre-trained or instruction-tuned). Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks. CLI Open the terminal and run ollama run llama3 API Example using curl: curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama3\", \"prompt\":\"Why is the sky blue?\" }' API documentation Model variants Instruct is fine-tuned for chat/dialogue use cases. Example: ollama run llama3 ollama run llama3:70b Pre-trained is the base model. Example: ollama run llama3:text ollama run llama3:70b-text References Introducing Meta Llama 3: The most capable openly available LLM to date Write Preview # Llama 3 The most capable openly available LLM to date. <img src=\"https://github.com/ollama/ollama/assets/3325447/15750d75-668c-42bd-aaf2-d0d203136d55\" width=\"660\" /> Meta Llama 3, a family of models developed by Meta Inc. are new state-of-the-art , available in both **8B** and **70B** parameter sizes (pre-trained or instruction-tuned). Llama 3 instruction-tuned models are fine-tuned and optimized for dialogue/chat use cases and outperform many of the available open-source chat models on common benchmarks. <img src=\"https://github.com/ollama/ollama/assets/3325447/8910aebc-cd9e-4d2d-b9c2-258b5ac3eeac\" /> <img src=\"https://github.com/ollama/ollama/assets/3325447/f6df22a6-fd54-4aa2-876b-2b9354821ec6\" /> ### CLI Open the terminal and run `ollama run llama3` ### API Example using curl: ```bash curl -X POST http://localhost:11434/api/generate -d '{ \"model\": \"llama3\", \"prompt\":\"Why is the sky blue?\" }' ``` [API documentation](https://github.com/ollama/ollama/blob/main/docs/api.md) ## Model variants **Instruct** is fine-tuned for chat/dialogue use cases. *Example:* `ollama run llama3` `oll",
      "variants": [
        {
          "tag": "llama3:latest",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:instruct",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:text",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-instruct-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q2_K",
          "size_text": "3.2GB",
          "size_bytes": 3435973836,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q3_K_S",
          "size_text": "3.7GB",
          "size_bytes": 3972844748,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q3_K_M",
          "size_text": "4.0GB",
          "size_bytes": 4294967296,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q3_K_L",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q4_0",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q4_1",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q4_K_S",
          "size_text": "4.7GB",
          "size_bytes": 5046586572,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q4_K_M",
          "size_text": "4.9GB",
          "size_bytes": 5261334937,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q5_0",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q5_1",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q5_K_S",
          "size_text": "5.6GB",
          "size_bytes": 6012954214,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q5_K_M",
          "size_text": "5.7GB",
          "size_bytes": 6120328396,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q6_K",
          "size_text": "6.6GB",
          "size_bytes": 7086696038,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-q8_0",
          "size_text": "8.5GB",
          "size_bytes": 9126805504,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:8b-text-fp16",
          "size_text": "16GB",
          "size_bytes": 17179869184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-instruct-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q2_K",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q3_K_S",
          "size_text": "31GB",
          "size_bytes": 33285996544,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q3_K_M",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q3_K_L",
          "size_text": "37GB",
          "size_bytes": 39728447488,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q4_0",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q4_1",
          "size_text": "44GB",
          "size_bytes": 47244640256,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q4_K_S",
          "size_text": "40GB",
          "size_bytes": 42949672960,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q4_K_M",
          "size_text": "43GB",
          "size_bytes": 46170898432,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q5_0",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q5_1",
          "size_text": "53GB",
          "size_bytes": 56908316672,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q5_K_S",
          "size_text": "49GB",
          "size_bytes": 52613349376,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q5_K_M",
          "size_text": "50GB",
          "size_bytes": 53687091200,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q6_K",
          "size_text": "58GB",
          "size_bytes": 62277025792,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-q8_0",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "8K",
          "input": "Text"
        },
        {
          "tag": "llama3:70b-text-fp16",
          "size_text": "141GB",
          "size_bytes": 151397597184,
          "context": "8K",
          "input": "Text"
        }
      ],
      "tags_count": 68
    },
    {
      "slug": "gpt-oss",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 6000000,
      "pulls_text": "6M \n  Downloads",
      "blurb": "gpt-oss OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases. cloud 20b 120b 5 Tags Updated 3 months ago",
      "name": "OpenAI’s open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.",
      "description": "Readme Welcome OpenAI’s gpt-oss! Ollama partners with OpenAI to bring its latest state-of-the-art open weight models to Ollama. The two models, 20B and 120B, bring a whole new local chat experience, and are designed for powerful reasoning, agentic tasks, and versatile developer use cases. Get started You can get started by downloading the latest Ollama version . The model can be downloaded directly in Ollama’s new app or via the terminal: ollama run gpt-oss:20b ollama run gpt-oss:120b Feature highlights Agentic capabilities: Use the models’ native capabilities for function calling, web browsing (Ollama is introducing built-in web search that can be optionally enabled), python tool calls, and structured outputs. Full chain-of-thought: Gain complete access to the model’s reasoning process, facilitating easier debugging and increased trust in outputs. Configurable reasoning effort: Easily adjust the reasoning effort (low, medium, high) based on your specific use case and latency needs. Fine-tunable: Fully customize models to your specific use case through parameter fine-tuning. Permissive Apache 2.0 license: Build freely without copyleft restrictions or patent risk—ideal for experimentation, customization, and commercial deployment. Quantization - MXFP4 format OpenAI utilizes quantization to reduce the memory footprint of the gpt-oss models. The models are post-trained with quantization of the mixture-of-experts (MoE) weights to MXFP4 format, where the weights are quantized to 4.25 bits per parameter. The MoE weights are responsible for 90+% of the total parameter count, and quantizing these to MXFP4 enables the smaller model to run on systems with as little as 16GB memory, and the larger model to fit on a single 80GB GPU. Ollama is supporting the MXFP4 format natively without additional quantizations or conversions. New kernels are developed for Ollama’s new engine to support the MXFP4 format. Ollama collaborated with OpenAI to benchmark against their reference implem",
      "variants": [
        {
          "tag": "gpt-oss:latest",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss:20b",
          "size_text": "14GB",
          "size_bytes": 15032385536,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss:120b",
          "size_text": "65GB",
          "size_bytes": 69793218560,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss:20b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "128K",
          "input": "Text"
        },
        {
          "tag": "gpt-oss:120b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "128K",
          "input": "Text"
        }
      ],
      "tags_count": 5
    },
    {
      "slug": "qwen3-coder",
      "capabilities": [
        "tools"
      ],
      "pulls": 2100000,
      "pulls_text": "2.1M \n  Downloads",
      "blurb": "qwen3-coder Alibaba's performant long context models for agentic and coding tasks. cloud 30b 480b 10 Tags Updated 3 months ago",
      "name": "Alibaba's performant long context models for agentic and coding tasks.",
      "description": "Readme Qwen3-Coder is the most agentic code model to date in the Qwen series. Get started 480B Cloud ollama run qwen3-coder:480b-cloud Local ollama run qwen3-coder:480b Running locally requires a minimum of 250GB of memory or unified memory. 30B ollama run qwen3-coder:30b Overview qwen3-coder:30b offers 30B total parameters with only 3.3B activated, delivering strong performance while maintaining efficiency. Exceptional agentic capabilities for real-world software engineering tasks through advanced long-horizon reinforcement learning on SWE-Bench and similar benchmarks. Long context support with 256K tokens natively and up to 1M tokens using extrapolation methods, optimized for repository-scale understanding. Scaled pretraining on 7.5T tokens (70% code ratio) while preserving strong general and mathematical abilities. Execution-driven reinforcement learning that significantly boosts code execution success rates across diverse real-world coding tasks. Reference Blog Write Preview ![Qwen 3 logo](/assets/library/qwen3/a5541098-87ba-4184-a5af-2b63312c2522) **Qwen3-Coder** is the most agentic code model to date in the Qwen series. ### Get started **480B** Cloud ``` ollama run qwen3-coder:480b-cloud ``` Local ``` ollama run qwen3-coder:480b ``` Running locally requires a minimum of 250GB of memory or unified memory. **30B** ``` ollama run qwen3-coder:30b ``` ### Overview `qwen3-coder:30b` offers 30B total parameters with only 3.3B activated, delivering strong performance while maintaining efficiency. - Exceptional agentic capabilities for real-world software engineering tasks through advanced long-horizon reinforcement learning on SWE-Bench and similar benchmarks. - Long context support with 256K tokens natively and up to 1M tokens using extrapolation methods, optimized for repository-scale understanding. - Scaled pretraining on 7.5T tokens (70% code ratio) while preserving strong general and mathematical abilities. - Execution-driven reinforcement learning that significa",
      "variants": [
        {
          "tag": "qwen3-coder:latest",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:30b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b",
          "size_text": "290GB",
          "size_bytes": 311385128960,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:30b-a3b-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:30b-a3b-q8_0",
          "size_text": "32GB",
          "size_bytes": 34359738368,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:30b-a3b-fp16",
          "size_text": "61GB",
          "size_bytes": 65498251264,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b-a35b-q4_K_M",
          "size_text": "290GB",
          "size_bytes": 311385128960,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b-a35b-q8_0",
          "size_text": "510GB",
          "size_bytes": 547608330240,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b-a35b-fp16",
          "size_text": "960GB",
          "size_bytes": 1030792151040,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-coder:480b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 10
    },
    {
      "slug": "qwen3-vl",
      "capabilities": [
        "multimodal",
        "reasoning",
        "thinking",
        "tools",
        "vision"
      ],
      "pulls": 1100000,
      "pulls_text": "1.1M \n  Downloads",
      "blurb": "qwen3-vl The most powerful -language model in the Qwen model family to date. cloud 2b 4b 8b 30b 32b 235b 59 Tags Updated 2 months ago",
      "name": "The most powerful vision-language model in the Qwen model family to date.",
      "description": "Readme Qwen3-VL models require Ollama 0.12.7 Qwen3-VL is the most powerful vision-language model in the Qwen family to date. In this generation, there are improvements to the model in many areas: its understanding and generating text, perceiving and reasoning about visual content, supporting longer context lengths, understanding spatial relationships and dynamic videos, or interacting with AI agents — Qwen3-VL shows clear and significant progress in every area. Models 2B ollama run qwen3-vl:2b 4B ollama run qwen3-vl:4b 8B ollama run qwen3-vl:8b 30B ollama run qwen3-vl:30b 32B ollama run qwen3-vl:32b 235B ollama run qwen3-vl:235b ollama run qwen3-vl:235b-cloud Key features Visual Agent Capabilities : Qwen3-VL can operate computer and mobile interfaces — recognize GUI elements, understand button functions, call tools, and complete tasks. It achieves top global performance on benchmarks like OS World, and using tools significantly improves its performance on fine-grained perception tasks. Superior Text-Centric Performance : Qwen3-VL employs early-stage joint pretraining of text and visual modalities, continuously strengthening its language capabilities. Its performance on text-based tasks matches that of Qwen3-235B-A22B-2507 — the flagship language model — making it a truly “text-grounded, multimodal powerhouse” for the next generation of vision-language models. Greatly Improved Visual Coding : It can now generate code from images or videos — for example, turning a design mockup into Draw.io, HTML, CSS, or JavaScript code — making “what you see is what you get” visual programming a reality. Much Better Spatial Understanding : 2D grounding from absolute coordinates to relative coordinates. It can judge object positions, viewpoint changes, and occlusion relationships. It supports 3D grounding, laying the foundation for complex spatial reasoning and embodied AI applications. Long Context & Long Video Understanding : All models natively support 256K tokens of context, expa",
      "variants": [
        {
          "tag": "qwen3-vl:latest",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-instruct",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-instruct-q4_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-instruct-q8_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-instruct-bf16",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-thinking",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-thinking-q4_K_M",
          "size_text": "1.9GB",
          "size_bytes": 2040109465,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-thinking-q8_0",
          "size_text": "2.6GB",
          "size_bytes": 2791728742,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:2b-thinking-bf16",
          "size_text": "4.3GB",
          "size_bytes": 4617089843,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-instruct",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-instruct-q4_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-instruct-q8_0",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-instruct-bf16",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-thinking",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-thinking-q4_K_M",
          "size_text": "3.3GB",
          "size_bytes": 3543348019,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-thinking-q8_0",
          "size_text": "5.1GB",
          "size_bytes": 5476083302,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:4b-thinking-bf16",
          "size_text": "8.9GB",
          "size_bytes": 9556302233,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-instruct",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-instruct-q4_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-instruct-q8_0",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-instruct-bf16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-thinking",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-thinking-q4_K_M",
          "size_text": "6.1GB",
          "size_bytes": 6549825126,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-thinking-q8_0",
          "size_text": "9.8GB",
          "size_bytes": 10522669875,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:8b-thinking-bf16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-instruct",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-instruct-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-instruct-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-instruct-bf16",
          "size_text": "62GB",
          "size_bytes": 66571993088,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-thinking",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-thinking-q4_K_M",
          "size_text": "20GB",
          "size_bytes": 21474836480,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-thinking-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:30b-a3b-thinking-bf16",
          "size_text": "62GB",
          "size_bytes": 66571993088,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-instruct",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-instruct-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-instruct-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-instruct-bf16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-thinking",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-thinking-q4_K_M",
          "size_text": "21GB",
          "size_bytes": 22548578304,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-thinking-q8_0",
          "size_text": "36GB",
          "size_bytes": 38654705664,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:32b-thinking-bf16",
          "size_text": "67GB",
          "size_bytes": 71940702208,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-instruct",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-instruct-q4_K_M",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-instruct-q8_0",
          "size_text": "251GB",
          "size_bytes": 269509197824,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-instruct-bf16",
          "size_text": "471GB",
          "size_bytes": 505732399104,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-thinking",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-thinking-q4_K_M",
          "size_text": "143GB",
          "size_bytes": 153545080832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-thinking-q8_0",
          "size_text": "251GB",
          "size_bytes": 269509197824,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-a22b-thinking-bf16",
          "size_text": "471GB",
          "size_bytes": 505732399104,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "qwen3-vl:235b-instruct-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 59
    },
    {
      "slug": "ministral-3",
      "capabilities": [
        "tools",
        "vision"
      ],
      "pulls": 255100,
      "pulls_text": "255.1K \n  Downloads",
      "blurb": "ministral-3 The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware. cloud 3b 8b 14b 16 Tags Updated 1 month ago",
      "name": "The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware.",
      "description": "Readme This model requires Ollama 0.13.1 , which is currently in pre-release. The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware. The Ministral 3 models offer the following capabilities: Vision: Enables the model to analyze images and provide insights based on visual content, in addition to text. Multilingual: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic. System Prompt: Maintains strong adherence and support for system prompts. Agentic: Offers best-in-class agentic capabilities with native function calling and JSON outputting. Edge-Optimized: Delivers best-in-class performance at a small scale, deployable anywhere. Apache 2.0 License: Open-source license allowing usage and modification for both commercial and non-commercial purposes. Large Context Window: Supports a 256k context window. Ministral 14B Ministral 8B Ministral 3B Write Preview <img src=\"/assets/library/ministral-3/83fa3859-d87f-492c-bd81-596cfbceeccb\" width=\"120\" /> > This model requires [Ollama 0.13.1](https://github.com/ollama/ollama/releases/tag/v0.13.1-rc1), which is currently in pre-release. The Ministral 3 family is designed for edge deployment, capable of running on a wide range of hardware. The Ministral 3 models offer the following capabilities: * Vision: Enables the model to analyze images and provide insights based on visual content, in addition to text. * Multilingual: Supports dozens of languages, including English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Arabic. * System Prompt: Maintains strong adherence and support for system prompts. * Agentic: Offers best-in-class agentic capabilities with native function calling and JSON outputting. * Edge-Optimized: Delivers best-in-class performance at a small scale, deployable anywhere. * Apache 2.0 License: Open-source license allowing usage and modification for both ",
      "variants": [
        {
          "tag": "ministral-3:latest",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:3b",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:8b",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:14b",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:3b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:3b-instruct-2512-q4_K_M",
          "size_text": "3.0GB",
          "size_bytes": 3221225472,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:3b-instruct-2512-q8_0",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:3b-instruct-2512-fp16",
          "size_text": "7.7GB",
          "size_bytes": 8267812044,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:8b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:8b-instruct-2512-q4_K_M",
          "size_text": "6.0GB",
          "size_bytes": 6442450944,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:8b-instruct-2512-q8_0",
          "size_text": "9.9GB",
          "size_bytes": 10630044057,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:8b-instruct-2512-fp16",
          "size_text": "18GB",
          "size_bytes": 19327352832,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:14b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:14b-instruct-2512-q4_K_M",
          "size_text": "9.1GB",
          "size_bytes": 9771050598,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:14b-instruct-2512-q8_0",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "ministral-3:14b-instruct-2512-fp16",
          "size_text": "28GB",
          "size_bytes": 30064771072,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 16
    },
    {
      "slug": "nemotron-3-nano",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools"
      ],
      "pulls": 110700,
      "pulls_text": "110.7K \n  Downloads",
      "blurb": "nemotron-3-nano Nemotron 3 Nano - A new Standard for Efficient, Open, and Intelligent Agentic Models cloud 30b 6 Tags Updated 1 month ago",
      "name": "Nemotron 3 Nano - A new Standard for Efficient, Open, and Intelligent Agentic Models",
      "description": "Readme Nemotron 3 Nano 30B ollama run nemotron-3-nano:30b Ollama’s Cloud ollama run nemotron-3-nano:30b-cloud Model Dates: September 2025 - December 2025 Data Freshness: The post-training data has a cutoff date of November 28, 2025. The pre-training data has a cutoff date of June 25, 2025. What is Nemotron? NVIDIA Nemotron™ is a family of open models with open weights, training data, and recipes, delivering leading efficiency and accuracy for building specialized AI agents. Nemotron 3 Nano is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response. The model’s reasoning capabilities can be configured through a flag in the chat template. If the user prefers the model to provide its final answer without intermediate reasoning traces, it can be configured to do so, albeit with a slight decrease in accuracy for harder prompts that require reasoning. Conversely, allowing the model to generate reasoning traces first generally results in higher-quality final solutions to queries and tasks. The model employs a hybrid Mixture-of-Experts (MoE) architecture, consisting of 23 Mamba-2 and MoE layers, along with 6 Attention layers. Each MoE layer includes 128 experts plus 1 shared expert, with 6 experts activated per token. The model has 3.5B active parameters and 30B parameters in total. The supported languages include: English, German, Spanish, French, Italian, and Japanese. Improved using Qwen. Reasoning Benchmark Evaluations Task NVIDIA-Nemotron-3-Nano-30B-A3B-BF16 Qwen3-30B-A3B-Thinking-2507 GPT-OSS-20B General Knowledge MMLU-Pro 78.3 80.9 75.0 Reasoning AIME25 (no tools) 89.1 85.0 91.7 AIME25 (with tools) 99.2 - 98.7 GPQA (no tools) 73.0 73.4 71.5 GPQA (with tools) 75.0 - 74.2 LiveCodeBench (v6 2025-08–2025-05) 68.3 66.0 61.0 SciCode (subtask) 33.3 33.0 34.0 HLE (no tools) 10.6 ",
      "variants": [
        {
          "tag": "nemotron-3-nano:latest",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "nemotron-3-nano:30b",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "nemotron-3-nano:30b-a3b-q4_K_M",
          "size_text": "24GB",
          "size_bytes": 25769803776,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "nemotron-3-nano:30b-a3b-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "nemotron-3-nano:30b-a3b-fp16",
          "size_text": "63GB",
          "size_bytes": 67645734912,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "nemotron-3-nano:30b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 6
    },
    {
      "slug": "devstral-small-2",
      "capabilities": [
        "thinking",
        "tools",
        "vision"
      ],
      "pulls": 95100,
      "pulls_text": "95.1K \n  Downloads",
      "blurb": "devstral-small-2 24B model that excels at using to explore codebases, editing multiple files and power software engineering agents. cloud 24b 6 Tags Updated 1 month ago",
      "name": "24B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.",
      "description": "Readme Note: this model requires Ollama 0.13.3 or later. Download Ollama Devstral Small 2 Devstral is an agentic LLM for software engineering tasks. Devstral 2 models excel at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench. 24B model ollama run devstral-small-2 Key Features The Devstral 2 Instruct model offers the following capabilities: Agentic Coding : Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents. Improved Performance : Devstral 2 is a step-up compared to its predecessors. Better Generalization : Generalises better to diverse prompts and coding environments. Use Cases AI Code Assistants, Agentic Coding, and Software Engineering Tasks. Leveraging advanced AI capabilities for complex tool integration and deep codebase understanding in coding environments. Benchmark Results Model/Benchmark Size (B Tokens) SWE Bench Verified SWE Bench Multilingual Terminal Bench Devstral 2 123 72.2% 61.3% 40.5% Devstral Small 2 24 65.8% 51.6% 32.0% DeepSeek v3.2 671 73.1% 70.2% 46.4% Kimi K2 Thinking 1000 71.3% 61.1% 35.7% MiniMax M2 230 69.4% 56.5% 30.0% GLM 4.6 455 68.0% – 40.5% Qwen 3 Coder Plus 480 69.6% 54.7% 37.5% Gemini 3 Pro – 76.2% – 54.2% Claude Sonnet 4.5 – 77.2% 68.0% 42.8% GPT 5.1 Codex Max – 77.9% – 58.1% GPT 5.1 Codex High – 73.7% – 52.8% License Devstral Small 2 - 24B Apache 2.0 Reference Devstral 2 Write Preview <img src=\"/assets/library/devstral-2/22065d6d-626a-4fc8-af4c-2efe10844651\" width=\"72\" /> > Note: this model requires Ollama 0.13.3 or later. [Download Ollama](https://ollama.com/download) # Devstral Small 2 Devstral is an agentic LLM for software engineering tasks. **Devstral 2** models excel at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench. **[24B model](https://ollama.com/library/devstra",
      "variants": [
        {
          "tag": "devstral-small-2:latest",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "384K",
          "input": "Text"
        },
        {
          "tag": "devstral-small-2:24b",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "384K",
          "input": "Text"
        },
        {
          "tag": "devstral-small-2:24b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "devstral-small-2:24b-instruct-2512-q4_K_M",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "384K",
          "input": "Text"
        },
        {
          "tag": "devstral-small-2:24b-instruct-2512-q8_0",
          "size_text": "26GB",
          "size_bytes": 27917287424,
          "context": "384K",
          "input": "Text"
        },
        {
          "tag": "devstral-small-2:24b-instruct-2512-fp16",
          "size_text": "48GB",
          "size_bytes": 51539607552,
          "context": "384K",
          "input": "Text"
        }
      ],
      "tags_count": 6
    },
    {
      "slug": "olmo-3",
      "capabilities": [
        "reasoning",
        "thinking"
      ],
      "pulls": 80200,
      "pulls_text": "80.2K \n  Downloads",
      "blurb": "olmo-3 Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets. 7b 32b 15 Tags Updated 1 month ago",
      "name": "Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets.",
      "description": "Readme Olmo 3, a new family of 7B and 32B models in both Instruct and Think variants. It has long chain-of-thought thinking to improve reasoning tasks like math and coding. Olmo is a series of Open language models designed to enable the science of language models. These models are pre-trained on the Dolma 3 dataset and post-trained on the Dolci datasets. Allen AI team is releasing all code, checkpoints, logs, and associated training details. Models Olmo 3 Instruct 7B ollama run olmo-3:7b-instruct Olmo 3 Think 7B ollama run olmo-3:7b-think Olmo 3 Think 32B ollama run olmo-3:32b-think Evaluation Olmo 3 Instruct 7B Benchmark Olmo3 Instruct 7B Qwen 3 8B (no reasoning) Qwen 3 VL 8B Instruct Qwen 2.5 7B Olmo 2 7B Instruct Apertus 8B Instruct Granite 3.3 8B Instruct MATH 87.3 82.3 91.6 71 30.1 21.9 67.3 AIME 2024 44.3 26.2 55.1 11.3 1.3 0.5 7.3 AIME 2025 32.5 21.7 43.3 6.3 0.4 0.2 6.3 OMEGA 28.9 20.5 32.3 13.7 5.2 5.0 10.7 BigBenchHard 71.2 73.7 85.6 68.8 43.8 42.2 61.2 ZebraLogic 32.9 25.4 64.3 10.7 5.3 5.3 17.6 AGI Eval English 64.4 76 84.5 69.8 56.1 50.8 64.0 HumanEvalPlus 77.2 79.8 82.9 74.9 25.8 34.4 64.0 MBPP+ 60.2 64.4 66.3 62.6 40.7 42.1 54.0 LiveCodeBench v3 29.5 53.2 55.9 34.5 7.2 7.8 11.5 IFEval 85.6 86.3 87.8 73.4 72.2 71.4 77.5 IFBench 32.3 29.3 34 28.4 26.7 22.1 22.3 MMLU 69.1 80.4 83.6 77.2 61.6 62.7 63.5 PopQA 14.1 20.4 26.5 21.5 25.5 25.5 28.9 GPQA 40.4 44.6 51.1 35.6 31.3 28.8 33.0 AlpacaEval 2 LC 40.9 49.8 73.5 23 18.3 8.1 28.6 SimpleQA 79.3 79 90.3 78 – – – LitQA2 38.2 39.6 30.7 29.8 – – – BFCL 49.8 60.2 66.2 55.8 – – – Safety 87.3 78 80.2 73.4 93.1 72.2 73.7 Olmo 3 Think 7B Benchmark Olmo 3 Think 7B OpenThinker3-7B Nemotron-Nano-9B-v2 DeepSeek-R1-Distill-Qwen-7B Qwen 3 8B (reasoning) Qwen 3 VL 8B Thinker OpenReasoning Nemotron 7B MATH 95.1 94.5 94.4 87.9 95.1 95.2 94.6 AIME 2024 71.6 67.7 72.1 54.9 74.0 70.9 77.0 AIME 2025 64.6 57.2 58.9 40.2 67.8 61.5 73.1 OMEGA 37.8 38.4 42.4 28.5 43.4 38.1 43.2 BBH 86.6 77.1 86.2 73.5 84.4 86.8 81.3 ZebraLogic 66.5 ",
      "variants": [
        {
          "tag": "olmo-3:latest",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:32b",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b-instruct",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b-instruct-q4_K_M",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b-instruct-q8_0",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b-instruct-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b-think",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b-think-q4_K_M",
          "size_text": "4.5GB",
          "size_bytes": 4831838208,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b-think-q8_0",
          "size_text": "7.8GB",
          "size_bytes": 8375186227,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:7b-think-fp16",
          "size_text": "15GB",
          "size_bytes": 16106127360,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:32b-think",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:32b-think-q4_K_M",
          "size_text": "19GB",
          "size_bytes": 20401094656,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:32b-think-q8_0",
          "size_text": "34GB",
          "size_bytes": 36507222016,
          "context": "64K",
          "input": "Text"
        },
        {
          "tag": "olmo-3:32b-think-fp16",
          "size_text": "64GB",
          "size_bytes": 68719476736,
          "context": "64K",
          "input": "Text"
        }
      ],
      "tags_count": 15
    },
    {
      "slug": "devstral-2",
      "capabilities": [
        "thinking",
        "tools"
      ],
      "pulls": 46400,
      "pulls_text": "46.4K \n  Downloads",
      "blurb": "devstral-2 123B model that excels at using to explore codebases, editing multiple files and power software engineering agents. cloud 123b 6 Tags Updated 1 month ago",
      "name": "123B model that excels at using tools to explore codebases, editing multiple files and power software engineering agents.",
      "description": "Readme Devstral 2 Devstral is an agentic LLM for software engineering tasks. Devstral 2 excels at using tools to explore codebases, editing multiple files and power software engineering agents. The model achieves remarkable performance on SWE-bench. 123B model ollama run devstral-2 Ollama’s Cloud ollama run devstral-2:123b-cloud Key Features The Devstral 2 Instruct model offers the following capabilities: Agentic Coding : Devstral is designed to excel at agentic coding tasks, making it a great choice for software engineering agents. Improved Performance : Devstral 2 is a step-up compared to its predecessors. Better Generalization : Generalises better to diverse prompts and coding environments. Use Cases AI Code Assistants, Agentic Coding, and Software Engineering Tasks. Leveraging advanced AI capabilities for complex tool integration and deep codebase understanding in coding environments. Benchmark Results Model/Benchmark Size (B Tokens) SWE Bench Verified SWE Bench Multilingual Terminal Bench Devstral 2 123 72.2% 61.3% 40.5% Devstral Small 2 24 65.8% 51.6% 32.0% DeepSeek v3.2 671 73.1% 70.2% 46.4% Kimi K2 Thinking 1000 71.3% 61.1% 35.7% MiniMax M2 230 69.4% 56.5% 30.0% GLM 4.6 455 68.0% – 40.5% Qwen 3 Coder Plus 480 69.6% 54.7% 37.5% Gemini 3 Pro – 76.2% – 54.2% Claude Sonnet 4.5 – 77.2% 68.0% 42.8% GPT 5.1 Codex Max – 77.9% – 58.1% GPT 5.1 Codex High – 73.7% – 52.8% License Devstral 2 - 123B Modified MIT License Attribution notice: 2025 - Mistral AI Permission is hereby granted, free of charge, to any person obtaining a copy of the weights of this model and associated documentation files (the “Model”), to deal in the Model without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Model, and to permit persons to whom the Model is furnished to do so, subject to the following conditions: The above attribution notice and this permission notice shall be included in all copies or ",
      "variants": [
        {
          "tag": "devstral-2:latest",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "devstral-2:123b",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "devstral-2:123b-cloud",
          "size_text": null,
          "size_bytes": null,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "devstral-2:123b-instruct-2512-q4_K_M",
          "size_text": "75GB",
          "size_bytes": 80530636800,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "devstral-2:123b-instruct-2512-q8_0",
          "size_text": "133GB",
          "size_bytes": 142807662592,
          "context": "256K",
          "input": "Text"
        },
        {
          "tag": "devstral-2:123b-instruct-2512-fp16",
          "size_text": "250GB",
          "size_bytes": 268435456000,
          "context": "256K",
          "input": "Text"
        }
      ],
      "tags_count": 6
    },
    {
      "slug": "functiongemma",
      "capabilities": [
        "tools"
      ],
      "pulls": 38200,
      "pulls_text": "38.2K \n  Downloads",
      "blurb": "functiongemma FunctionGemma is a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling. 270m 4 Tags Updated 4 weeks ago",
      "name": "FunctionGemma is a specialized version of Google's Gemma 3 270M model fine-tuned explicitly for function calling.",
      "description": "Readme Requires Ollama v0.13.5 or later FunctionGemma FunctionGemma is a lightweight, open model from Google, built as a foundation for creating your own specialized function calling models. The model is well suited for text-only function calling. The uniquely small size makes it possible to deploy in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. FunctionGemma is not intended for use as a direct dialogue model, and is designed to be highly performant after further fine-tuning, as is typical of models this size. Built on the Gemma 3 270M model and with the same research and technology used to create the Gemini models, FunctionGemma has been trained specifically for function calling. The model has the same architecture as Gemma 3, but uses a different chat format. Furthermore, akin to the base Gemma 270M, the model has been optimized to be extremely versatile, performant on a variety of hardware in single turn scenarios, but should be finetuned on single turn or multiturn task specific data to achieve best accuracy in specific domains. Examples Python Run the python example with uv run tool.py # /// script # requires-python = \">=3.11\" # dependencies = [ # \"ollama\", # \"rich\", # ] # /// \"\"\" Single tool, single turn example. Run with: uv run tool.py \"\"\" import json from rich import print from ollama import chat model = 'functiongemma' def get_weather(city: str) -> str: \"\"\" Get the current weather for a city. Args: city: The name of the city Returns: A string describing the weather \"\"\" return json.dumps({'city': city, 'temperature': 22, 'unit': 'celsius', 'condition': 'sunny'}) messages = [{'role': 'user', 'content': 'What is the weather in Paris?'}] print('Prompt:', messages[0]['content']) response = chat(model, messages=messages, tools=[get_weather]) if response.message.tool_calls: tool = response.message.tool_calls[0] print(f'",
      "variants": [
        {
          "tag": "functiongemma:latest",
          "size_text": "301MB",
          "size_bytes": 315621376,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "functiongemma:270m",
          "size_text": "301MB",
          "size_bytes": 315621376,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "functiongemma:270m-it-q8_0",
          "size_text": "301MB",
          "size_bytes": 315621376,
          "context": "32K",
          "input": "Text"
        },
        {
          "tag": "functiongemma:270m-it-fp16",
          "size_text": "552MB",
          "size_bytes": 578813952,
          "context": "32K",
          "input": "Text"
        }
      ],
      "tags_count": 4
    },
    {
      "slug": "gemini-3-flash-preview",
      "capabilities": [
        "reasoning",
        "thinking",
        "tools",
        "vision"
      ],
      "pulls": 36700,
      "pulls_text": "36.7K \n  Downloads",
      "blurb": "gemini-3-flash-preview Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost. cloud 2 Tags Updated 3 weeks ago",
      "name": "Gemini 3 Flash offers frontier intelligence built for speed at a fraction of the cost.",
      "description": "Readme Model Gemini 3 Flash Preview on Ollama’s cloud: ollama run gemini-3-flash-preview:cloud Benchmarks Gemini 3 Flash demonstrates that speed and scale don’t have to come at the cost of intelligence. It delivers frontier performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity’s Last Exam (33.7% without tools), rivaling larger frontier models, and significantly outperforming even the best 2.5 model, Gemini 2.5 Pro, across a number of benchmarks. It also reaches state-of-the-art performance with an impressive score of 81.2% on MMMU Pro, comparable to Gemini 3 Pro. Write Preview ![image.png](/assets/library/gemini-3-flash-preview/cc29c7e5-1edc-44bc-9710-1fd5885799fd) ### Model Gemini 3 Flash Preview on Ollama's cloud: ``` ollama run gemini-3-flash-preview:cloud ``` ### Benchmarks Gemini 3 Flash demonstrates that speed and scale don’t have to come at the cost of intelligence. It delivers frontier performance on PhD-level reasoning and knowledge benchmarks like GPQA Diamond (90.4%) and Humanity’s Last Exam (33.7% without tools), rivaling larger frontier models, and significantly outperforming even the best 2.5 model, Gemini 2.5 Pro, across a number of benchmarks. It also reaches state-of-the-art performance with an impressive score of 81.2% on MMMU Pro, comparable to Gemini 3 Pro. ![](/assets/library/gemini-3-flash-preview/cf40550d-c81b-4fcf-aa2c-9473efa2d028) Paste, drop or click to upload images (.png, .jpeg, .jpg, .svg, .gif)",
      "variants": [
        {
          "tag": "gemini-3-flash-preview:latest",
          "size_text": null,
          "size_bytes": null,
          "context": null,
          "input": "Text"
        },
        {
          "tag": "gemini-3-flash-preview:cloud",
          "size_text": null,
          "size_bytes": null,
          "context": null,
          "input": "Text"
        }
      ],
      "tags_count": 2
    }
  ]
}